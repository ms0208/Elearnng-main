# -*- coding: utf-8 -*-
"""Recommendation_LGBM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TQVXb-x7EznJsQmukMYRrr_3dD_C7Y8g
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import accuracy_score, classification_report
from sklearn.pipeline import make_pipeline
from sklearn.compose import ColumnTransformer
from lightgbm import LGBMClassifier
import lightgbm as lgb
import joblib
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns

# Set display options
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 1000)
sns.set_style('whitegrid')

def load_data():
    """Load and validate the input datasets from the uploaded files"""
    print(f"[{datetime.now()}] Loading data...")

    try:
        learner_profiles = pd.read_json('/content/updated_learner_profiles.json')
        courses = pd.read_json('/content/realistic_course_data.json')
        interactions = pd.read_json('/content/realistic_interaction_data.json')

        # Basic validation
        assert not learner_profiles.empty, "Learner profiles is empty"
        assert not courses.empty, "Courses data is empty"
        assert not interactions.empty, "Interactions data is empty"

        print("Data loaded successfully:")
        print(f"- Learner profiles: {learner_profiles.shape}")
        print(f"- Courses: {courses.shape}")
        print(f"- Interactions: {interactions.shape}")

        return learner_profiles, courses, interactions

    except Exception as e:
        print(f"Error loading data: {str(e)}")
        raise

# Load the data
learner_profiles, courses, interactions = load_data()

# Display sample data
print("\nSample learner profiles:")
display(learner_profiles.head(2))

print("\nSample courses:")
display(courses.head(2))

print("\nSample interactions:")
display(interactions.head(2))

# Basic statistics
print("\nInteractions statistics:")
print(interactions.describe())

print("\nCourse ratings distribution:")
print(courses['CourseRating'].value_counts().sort_index())

# Visualize rating distribution
plt.figure(figsize=(10, 6))
sns.countplot(x='CourseRating', data=courses)
plt.title('Distribution of Course Ratings')
plt.show()

def preprocess_data(learner_profiles, courses, interactions):
    """Merge and preprocess the datasets"""
    print(f"\n[{datetime.now()}] Preprocessing data...")

    # Merge datasets, bringing 'CourseRating' into interactions
    interactions = interactions.merge(courses[['CourseID', 'CourseRating']], on='CourseID', how='left')
    # Rename 'CourseRating' to 'Rating' in interactions
    interactions.rename(columns={'CourseRating': 'Rating'}, inplace=True)

    merged_df = (
        interactions.merge(courses, on='CourseID')
                   .merge(learner_profiles, on='UserID')
    )

    # Create text features
    merged_df['content_features'] = (
        merged_df['Description'].fillna('') + ' ' +
        merged_df['Tags'].fillna('') + ' ' +
        merged_df['Category'].fillna('')
    )

    # Target variable - high rating (>=4)
    merged_df['target'] = (merged_df['Rating'] >= 4).astype(int)

    # Add user engagement features
    user_engagement = interactions.groupby('UserID').agg(
        total_courses_taken=('CourseID', 'count'),
        avg_rating=('Rating', 'mean')
    ).reset_index()

    merged_df = merged_df.merge(user_engagement, on='UserID')

    print(f"Merged dataset shape: {merged_df.shape}")
    print("Target distribution:")
    print(merged_df['target'].value_counts())

    return merged_df
preprocessed_df = preprocess_data(learner_profiles, courses, interactions)

def create_feature_pipeline():
    """Create the feature engineering pipeline"""
    print(f"\n[{datetime.now()}] Creating feature pipeline...")

    # Text features pipeline
    text_pipeline = make_pipeline(
        TfidfVectorizer(
            stop_words='english',
            max_features=1000,
            ngram_range=(1, 2),
            min_df=5,
            max_df=0.9
        )
    )

    # Categorical features pipeline
    cat_pipeline = make_pipeline(
        OneHotEncoder(handle_unknown='ignore', sparse_output=False)
    )

    # Combine features
    preprocessor = ColumnTransformer(
        transformers=[
            ('text', text_pipeline, 'content_features'),
            ('cat', cat_pipeline, ['UserID', 'CourseID']),
            ('num', 'passthrough', ['total_courses_taken', 'avg_rating'])
        ],
        remainder='drop'
    )

    return preprocessor

# Create the feature pipeline
preprocessor = create_feature_pipeline()

def train_lightgbm(X_train, y_train):
    """Train and optimize LightGBM model"""
    print(f"\n[{datetime.now()}] Training LightGBM model...")

    model = LGBMClassifier(
        n_estimators=200,
        max_depth=10,
        learning_rate=0.05,
        subsample=0.8,
        colsample_bytree=0.8,
        reg_alpha=0.1,
        reg_lambda=0.1,
        n_jobs=-1,
        random_state=42,
        importance_type='gain',
        metric='binary_logloss',
        verbose=1
    )

    model.fit(
        X_train,
        y_train,
        eval_set=[(X_train, y_train)],
        callbacks=[lgb.early_stopping(stopping_rounds=20, verbose=True),
                   lgb.log_evaluation(period=1, show_stdv=True)],  # Add lgb.log_evaluation for verbose output
        # verbose=1  # Remove verbose from here as well
    )

    return model

# Prepare data for training
X = preprocessed_df[['content_features', 'UserID', 'CourseID', 'total_courses_taken', 'avg_rating']]
y = preprocessed_df['target']

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Transform features
print("\nTransforming features...")
X_train_transformed = preprocessor.fit_transform(X_train)
X_test_transformed = preprocessor.transform(X_test)

# Train model
model = train_lightgbm(X_train_transformed, y_train)

def evaluate_model(model, X_test, y_test):
    """Evaluate model performance"""
    print(f"\n[{datetime.now()}] Evaluating model...")

    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1]

    print("\nModel Evaluation:")
    print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))

    # Feature importance
    if hasattr(model, 'feature_importances_'):
        print("\nTop 20 Features by Importance:")
        feature_names = (
            preprocessor.named_transformers_['text'].named_steps['tfidfvectorizer'].get_feature_names_out().tolist() +
            preprocessor.named_transformers_['cat'].named_steps['onehotencoder'].get_feature_names_out().tolist() +
            ['total_courses_taken', 'avg_rating']
        )

        importances = pd.Series(model.feature_importances_,
                              index=feature_names)
        top_features = importances.sort_values(ascending=False).head(20)

        plt.figure(figsize=(10, 6))
        sns.barplot(x=top_features.values, y=top_features.index)
        plt.title('Top 20 Features by Importance')
        plt.xlabel('Importance Score')
        plt.show()

# Evaluate the model
evaluate_model(model, X_test_transformed, y_test)

def recommend_courses(user_id, model, preprocessor, courses, interactions, top_n=10):
    """Generate personalized course recommendations"""
    print(f"\n[{datetime.now()}] Generating recommendations for user {user_id}...")

    # Get user's taken courses
    user_courses = interactions[interactions['UserID'] == user_id]['CourseID']

    # Get candidate courses (not taken by user)
    candidate_courses = courses[~courses['CourseID'].isin(user_courses)].copy()

    if candidate_courses.empty:
        return pd.DataFrame({"Message": ["No new courses to recommend for this user."]})

    # ----> FIX: Merge 'CourseRating' into interactions before calculating engagement features
    interactions_with_rating = interactions.merge(courses[['CourseID', 'CourseRating']], on='CourseID', how='left')
    interactions_with_rating.rename(columns={'CourseRating': 'Rating'}, inplace=True)

    # Get user engagement features
    user_engagement = interactions_with_rating.groupby('UserID').agg(
        total_courses_taken=('CourseID', 'count'),
        avg_rating=('Rating', 'mean')
    ).reset_index()

    user_stats = user_engagement[user_engagement['UserID'] == user_id]
    # ... (Rest of the function remains the same)

    # Prepare features for prediction
    candidate_courses['content_features'] = (
        candidate_courses['Description'].fillna('') + ' ' +
        candidate_courses['Tags'].fillna('') + ' ' +
        candidate_courses['Category'].fillna('')
    )

    candidate_courses['UserID'] = user_id
    candidate_courses = candidate_courses.merge(user_stats, on='UserID')

    # Transform features
    X = preprocessor.transform(candidate_courses)

    # Get predictions
    candidate_courses['pred_score'] = model.predict_proba(X)[:, 1]

    # Get top recommendations
    recommendations = (
        candidate_courses.sort_values('pred_score', ascending=False)
        .head(top_n)
        [['CourseID', 'CourseTitle', 'pred_score']]
        .reset_index(drop=True)
    )

    return recommendations

print("\nSaving model artifacts...")
joblib.dump(model, 'course_recommender_lgbm.pkl')
joblib.dump(preprocessor, 'preprocessor_lgbm.pkl')
print("Model and preprocessor saved successfully.")

sample_user = interactions['UserID'].sample(1).iloc[0]
print(f"\nGenerating recommendations for sample user {sample_user}:")

recommendations = recommend_courses(
    sample_user, model, preprocessor, courses, interactions
)

print("\nTop Recommendations:")
display(recommendations)

plt.figure(figsize=(10, 6))
sns.barplot(x='pred_score', y='CourseTitle', data=recommendations)
plt.title(f'Top Recommended Courses for User {sample_user}')
plt.xlabel('Prediction Score')
plt.ylabel('Course Title')
plt.tight_layout()
plt.show()